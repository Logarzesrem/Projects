{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98803e11-cdd9-4f19-8081-91720722d15f",
   "metadata": {},
   "source": [
    "Proszę napisać program, który będzie odgadywać język w jakim napisany jest tekst.\n",
    "\n",
    "Przydatny będzie program z zadania 3 (ngramy) oraz miary odległości (metryki):\n",
    "\n",
    "- kosinusowa (cosinusowa),\n",
    "- euklidesowa,\n",
    "- taksówkowa (Manhattan),\n",
    "- maksimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0286e269-b765-489e-81c6-fdb394eaf053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tarfile\n",
    "#with tarfile.open(\"teksty.tar\", \"r\") as tar:\n",
    "#    tar.extractall(path = \"teksty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d30bfc9-cef9-4c24-a52c-a1ec2ca2a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1546bd0-048c-4353-9682-bfc7a9e68736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = text.replace('\\\\', '')\n",
    "        text = re.sub(r\"[^a-zA-ZÅåÄäÖöÀàáÈèéíÓóúñÑüÜẞßĄąĆćĘęŁłNńŚśŻżŹźÙù]\", \"\", text).lower()\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c110243-08d5-4aa1-ade6-fc24c41acdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(folder_path):\n",
    "    full_text = \"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            text = read_text_from_file(file_path)\n",
    "            full_text += text + \" \" \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bf5171-1bbc-449a-93da-2f8589fc9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = {\n",
    "    \"english\": \"eng\",\n",
    "    \"finnish\": \"fin\",\n",
    "    \"german\": \"ger\",\n",
    "    \"italian\": \"ita\",\n",
    "    \"polish\": \"pol\",\n",
    "    \"spanish\": \"spa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646570db-f45a-4dfb-88ee-c8961bedef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English concatenate: harrypotterandthesorcerersstonechapteronetheboywholivedmrandmrsdursleyofnumberfourprivetdrivewerepro\n",
      "Finnish concatenate: oalussaloijumalataivaanjamaanojamaaoliautiojatyhjäjapimeysolisyvyydenpäälläjajumalanhenkiliikkuivett\n",
      "German concatenate: theodormommsenroemischegeschichtezweitesbuchvonderabschaffungdesroemischenkoenigtumsbiszureinigungit\n",
      "Italian concatenate: wumingagilbertocentinoncnessundopoguerraglistoltichiamavanopaceilsempliceallontanarsidelfronteglisto\n",
      "Polish concatenate: polskarzeczpospolitapolskapaństwopolozoneweuropiesrodkowejmiedzybaltykiemnapolnocyakarpatamiisudetam\n",
      "Spanish concatenate: lanovelaconstadedosparteslaprimeraelingeniosohidalgodonquijotedelamanchafuepublicadaenlasegundasegun\n"
     ]
    }
   ],
   "source": [
    "for language, folder_path in folders.items():\n",
    "    concatenated_text = concatenate(folder_path)\n",
    "    print(f\"{language.capitalize()} concatenate: {concatenated_text[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ec2a6e3-384c-4139-b3d5-0c3992e25a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f773084-1d49-47f8-b485-2fbac701f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_profile(profile):\n",
    "    total = sum(profile.values())\n",
    "    if total == 0:\n",
    "        return profile\n",
    "    return {key: value / total for key, value in profile.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4b2662a-8d8d-4d54-a08e-32838872d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_ngrams = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cdb37d0-5a90-4133-90ac-0958956d7264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "n = 1: [('h', 133727), ('a', 163084), ('r', 132895), ('y', 50986), ('p', 34460), ('o', 156195), ('t', 170508), ('e', 238949), ('n', 131455), ('d', 100850)]\n",
      "n = 2: [('ha', 30038), ('ar', 24922), ('rr', 10863), ('ry', 13267), ('yp', 1363), ('po', 4261), ('ot', 10630), ('tt', 11960), ('te', 13878), ('er', 33580)]\n",
      "n = 3: [('har', 9224), ('arr', 8767), ('rry', 8620), ('ryp', 507), ('ypo', 436), ('pot', 977), ('ott', 1809), ('tte', 2431), ('ter', 4876), ('era', 1912)]\n",
      "\n",
      "\n",
      "Finnish:\n",
      "n = 1: [('o', 175118), ('a', 422980), ('l', 187322), ('u', 156619), ('s', 238634), ('i', 348872), ('j', 88804), ('m', 106915), ('t', 282189), ('v', 72103)]\n",
      "n = 2: [('oa', 3373), ('al', 29943), ('lu', 6766), ('us', 10563), ('ss', 15849), ('sa', 41107), ('lo', 5499), ('oi', 25232), ('ij', 9433), ('ju', 8056)]\n",
      "n = 3: [('oal', 81), ('alu', 931), ('lus', 486), ('uss', 927), ('ssa', 9859), ('sal', 2576), ('alo', 1467), ('loi', 2253), ('oij', 271), ('iju', 752)]\n",
      "\n",
      "\n",
      "German:\n",
      "n = 1: [('t', 237693), ('h', 192673), ('e', 769639), ('o', 116756), ('d', 225999), ('r', 303265), ('m', 99472), ('s', 285694), ('n', 427089), ('i', 334530)]\n",
      "n = 2: [('th', 6770), ('he', 53790), ('eo', 2434), ('od', 4339), ('do', 3898), ('or', 15938), ('rm', 7791), ('mo', 3824), ('om', 7043), ('mm', 6551)]\n",
      "n = 3: [('the', 1393), ('heo', 330), ('eod', 277), ('odo', 147), ('dor', 632), ('orm', 661), ('rmo', 827), ('mom', 59), ('omm', 2066), ('mms', 48)]\n",
      "\n",
      "\n",
      "Italian:\n",
      "n = 1: [('w', 587), ('u', 49792), ('m', 42373), ('i', 170557), ('n', 113626), ('g', 30449), ('a', 182913), ('l', 97166), ('b', 17458), ('e', 173421)]\n",
      "n = 2: [('wu', 4), ('um', 801), ('mi', 7414), ('in', 20232), ('ng', 3493), ('ga', 2568), ('ag', 6376), ('gi', 6411), ('il', 13667), ('lb', 973)]\n",
      "n = 3: [('wum', 3), ('umi', 220), ('min', 1469), ('ing', 1304), ('nga', 277), ('gag', 88), ('agi', 1184), ('gil', 151), ('ilb', 456), ('lbe', 248)]\n",
      "\n",
      "\n",
      "Polish:\n",
      "n = 1: [('p', 3936), ('o', 10394), ('l', 5250), ('s', 6408), ('k', 4551), ('a', 11084), ('r', 5910), ('z', 6680), ('e', 9587), ('c', 4775)]\n",
      "n = 2: [('po', 1929), ('ol', 1397), ('ls', 691), ('sk', 1127), ('ka', 754), ('ar', 1030), ('rz', 1023), ('ze', 1349), ('ec', 562), ('cz', 1274)]\n",
      "n = 3: [('pol', 893), ('ols', 582), ('lsk', 435), ('ska', 248), ('kar', 61), ('arz', 113), ('rze', 555), ('zec', 145), ('ecz', 168), ('czp', 38)]\n",
      "\n",
      "\n",
      "Spanish:\n",
      "n = 1: [('l', 162112), ('a', 310976), ('n', 180222), ('o', 303847), ('v', 37456), ('e', 381954), ('c', 98243), ('s', 252198), ('t', 118838), ('d', 159515)]\n",
      "n = 2: [('la', 42378), ('an', 31819), ('no', 24930), ('ov', 9523), ('ve', 7030), ('el', 56070), ('ac', 18182), ('co', 25626), ('on', 34076), ('ns', 7908)]\n",
      "n = 3: [('lan', 3144), ('ano', 4774), ('nov', 426), ('ove', 1115), ('vel', 229), ('ela', 11865), ('lac', 4358), ('aco', 4170), ('con', 12539), ('ons', 2586)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for language, folder_path in folders.items():\n",
    "    concatenated_text = concatenate(folder_path)\n",
    "    \n",
    "    for n in range(1, 4):\n",
    "        ngrams = generate_ngrams(concatenated_text, n)\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            language_ngrams[language][n][ngram] += 1\n",
    "\n",
    "    print(f\"{language.capitalize()}:\")\n",
    "    for n in range(1, 4):\n",
    "        print(f\"n = {n}: {list(language_ngrams[language][n].items())[:10]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "for language in language_ngrams:\n",
    "    for n in language_ngrams[language]:\n",
    "        language_ngrams[language][n] = normalize_profile(language_ngrams[language][n])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56e7aed0-5ce1-4c68-bd71-b014a2df1bbe",
   "metadata": {},
   "source": [
    "for language, folder_path in folders.items():\n",
    "    concatenated_text = concatenate(folder_path)\n",
    "    \n",
    "    for n in range(1, 4):\n",
    "        ngrams = generate_ngrams(concatenated_text, n)\n",
    "        \n",
    "        for ngram in ngrams:\n",
    "            language_ngrams[language][n][ngram] += 1\n",
    "\n",
    "    print(f\"{language.capitalize()}\")\n",
    "    for n in range(1, 4):\n",
    "        print(f\"n = {n}: {list(language_ngrams[language][n].items())[:10]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573963f6-bc73-4029-be84-3946e9c2b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(measure, test_dict, dict):\n",
    "    all_keys = set(test_dict.keys()).union(dict.keys())  # Wszystkie klucze\n",
    "\n",
    "    if measure == \"euclidean\":\n",
    "        squared_sum = sum((test_dict.get(ngram, 0) - dict.get(ngram, 0)) ** 2 for ngram in all_keys)\n",
    "        return math.sqrt(squared_sum)\n",
    "\n",
    "    elif measure == \"manhattan\":\n",
    "        absolute_sum = sum(abs(test_dict.get(ngram, 0) - dict.get(ngram, 0)) for ngram in all_keys)\n",
    "        return absolute_sum\n",
    "\n",
    "    elif measure == \"cosine\":\n",
    "        dot_product = sum(test_dict.get(ngram, 0) * dict.get(ngram, 0) for ngram in all_keys)\n",
    "        \n",
    "        magnitude_test = math.sqrt(sum(freq ** 2 for freq in test_dict.values()))\n",
    "        magnitude_dict = math.sqrt(sum(freq ** 2 for freq in dict.values()))\n",
    "        \n",
    "        if magnitude_test == 0 or magnitude_dict == 0:\n",
    "            return 0 \n",
    "        \n",
    "        cosine_similarity = dot_product / (magnitude_test * magnitude_dict)\n",
    "        return 1 - cosine_similarity\n",
    "\n",
    "    elif measure == \"maximum\":\n",
    "        max_diff = max(abs(test_dict.get(ngram, 0) - dict.get(ngram, 0)) for ngram in all_keys)\n",
    "        return max_diff\n",
    "\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2d106c-741d-4c10-ad9f-75bb0f8233de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_distances(distances):\n",
    "    min_distance = min(distances.values())\n",
    "    max_distance = max(distances.values())\n",
    "    scaled_distances = {}\n",
    "    \n",
    "    for language, value in distances.items():\n",
    "        #if max_distance - min_distance == 0:\n",
    "         #   scaled_value = 50.0\n",
    "        #else:\n",
    "        scaled_value = 1 * (value - min_distance) / (max_distance - min_distance)\n",
    "        scaled_distances[language] = round(scaled_value, 2)\n",
    "    \n",
    "    return scaled_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbff38af-89e1-478b-91f6-c5af312dd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_language(test_text, language_ngrams, n, metric):\n",
    "    test_ngrams = generate_ngrams(test_text, n)\n",
    "    test_profile = defaultdict(int)\n",
    "    for ngram in test_ngrams:\n",
    "        test_profile[ngram] += 1\n",
    "\n",
    "    test_profile = normalize_profile(test_profile)\n",
    "\n",
    "    distances = {}\n",
    "    for language, profiles in language_ngrams.items():\n",
    "        distance_value = distance(metric, test_profile, profiles[n])\n",
    "        distances[language] = round(distance_value, 2)\n",
    "\n",
    "    scaled_distances = scale_distances(distances)\n",
    "\n",
    "    predicted_language = min(scaled_distances, key=scaled_distances.get)\n",
    "    return predicted_language, scaled_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6220fd9c-4110-4c86-9010-443b4ebc5494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#selected_language = \"finnish\"  \n",
    "#selected_folder = folders[selected_language]\n",
    "#selected_file = \"fin1.txt\" \n",
    "\n",
    "#test_file_path = os.path.join(selected_folder, selected_file)\n",
    "\n",
    "#test_text = read_text_from_file(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61540afb-bb0f-458d-9a4c-eeaae2e66d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 3\n",
    "#metric = \"manhattan\"\n",
    "#predicted_language, scaled_distances = predict_language(test_text, language_ngrams, n, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901911f6-2d6d-4cd0-b340-86a9583e9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Predicted language: {predicted_language}\")\n",
    "#print(\"Dystans:\", scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449ed106-bfe2-4748-a47c-7e987fd6c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 3\n",
    "#metric = \"euclidean\"\n",
    "#predicted_language, scaled_distances = predict_language(test_text, language_ngrams, n, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3d3a2fb-e74e-46fb-babe-4945806cd5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Predicted language: {predicted_language}\")\n",
    "#print(\"Dystans:\", scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "735b766c-4fe1-41d2-b55e-881de647b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 3\n",
    "#metric = \"maximum\"\n",
    "#predicted_language, scaled_distances = predict_language(test_text, language_ngrams, n, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47f1db0-9652-4938-a23a-a4c072e6cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Predicted language: {predicted_language}\")\n",
    "#print(\"Dystans:\", scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8f4a8a5-ee75-4850-b3cc-4ac9a20c49d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 3\n",
    "#metric = \"cosine\"\n",
    "#predicted_language, scaled_distances = predict_language(test_text, language_ngrams, n, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199ca1ff-0f8a-4812-aaee-9be1773dbaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Predicted language: {predicted_language}\")\n",
    "#print(\"Dystans:\", scaled_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a03cf1c-b74f-4fb3-93e2-a313a1f4558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_files(folders, language_ngrams, metrics, n, file_path = \"test.txt\"): #test.txt - plik, którego język chcemy odgadnąć\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    if file_path:\n",
    "        \n",
    "        test_text = read_text_from_file(file_path)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            predicted_language, distances = predict_language(test_text, language_ngrams, n, metric)\n",
    "\n",
    "            for compare_language, distance_value in distances.items():\n",
    "                file_results = {\n",
    "                    \"File\": os.path.basename(file_path),\n",
    "                    \"Original Language\": \"?\",\n",
    "                    \"Metric\": metric,\n",
    "                    \"Compared Language\": compare_language,\n",
    "                    #\"Distance\": round(distance_value, 2)\n",
    "                    \"Distance\": f\"{distance_value:.2f}\" #liczba miejsc po przecinku\n",
    "                }\n",
    "                results.append(file_results)\n",
    "                \n",
    "    else:\n",
    "        for language, folder_path in folders.items():\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "    \n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "                    test_text = read_text_from_file(file_path)\n",
    "    \n",
    "                    for metric in metrics:\n",
    "                        predicted_language, distances = predict_language(test_text, language_ngrams, n, metric)\n",
    "    \n",
    "                        for compare_language, distance_value in distances.items():\n",
    "                            file_results = {\n",
    "                                \"File\": filename,\n",
    "                                \"Original Language\": language,\n",
    "                                \"Metric\": metric,\n",
    "                                \"Compared Language\": compare_language,\n",
    "                                #\"Distance\": round(distance_value, 2)\n",
    "                                \"Distance\": f\"{distance_value:.2f}\" #liczba miejsc po przecinku\n",
    "                            }\n",
    "                            results.append(file_results)\n",
    "\n",
    "    with open(\"language_distances.csv\", \"w\", newline = \"\", encoding = \"utf-8\") as csvfile:\n",
    "        fieldnames = [\"File\", \"Original Language\", \"Metric\", \"Compared Language\", \"Distance\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "metrics = [\"euclidean\", \"manhattan\", \"cosine\", \"maximum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef53daf4-e6fa-4779-a90b-5f3326485457",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kot.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage_ngrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mprocess_all_files\u001b[1;34m(folders, language_ngrams, metrics, n, file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path:\n\u001b[1;32m----> 7\u001b[0m     test_text \u001b[38;5;241m=\u001b[39m \u001b[43mread_text_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m     10\u001b[0m         predicted_language, distances \u001b[38;5;241m=\u001b[39m predict_language(test_text, language_ngrams, n, metric)\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mread_text_from_file\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text_from_file\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      3\u001b[0m         text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kot.txt'"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "process_all_files(folders, language_ngrams, metrics, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
